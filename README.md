ğŸ§  Mental Health Text Classification using BERT
This project focuses on classifying user-generated text into mental health categories using Transformer-based models, specifically BERT, in a Kaggle-compatible environment. It explores preprocessing techniques, handling class imbalance, and model fine-tuning using Hugging Face Transformers.

ğŸ¯ Objective
To build a text classification model that can accurately detect different mental health conditions (e.g., anxiety, depression) from social media or forum text using state-of-the-art NLP techniques.

âš™ï¸ Tech Stack
Language: Python

Frameworks & Libraries:

pandas, numpy â€“ Data handling

matplotlib, seaborn â€“ Visualization

scikit-learn â€“ Evaluation & preprocessing

transformers (Hugging Face) â€“ BERT model and tokenizer

nltk â€“ NLP preprocessing (stopwords)

torch â€“ PyTorch backend for model training

imblearn â€“ Oversampling for class imbalance

ğŸ“ Dataset
Source: Combined Data.csv (loaded from Kaggle dataset)

Description: Text samples labeled with corresponding mental health conditions.

ğŸ§¼ Preprocessing
Removal of stopwords using nltk

Regex cleaning

Label encoding

Oversampling using RandomOverSampler to balance classes

ğŸ§  Model
Base model: bert-base-uncased (from Hugging Face Transformers)

Fine-tuned using Trainer and TrainingArguments classes

Tokenized using BertTokenizer

ğŸ“Š Evaluation
Classification report (Precision, Recall, F1-score)

Confusion Matrix
ğŸ“Œ Highlights
Uses RandomOverSampler to tackle imbalanced data

Completely offline mode for W&B logging

Utilizes Hugging Faceâ€™s easy-to-use Trainer API

Optimized for GPU runtime on Kaggle (e.g., Tesla T4)

ğŸš€ Future Improvements
Experiment with larger models like roberta-base, distilbert

Deploy via REST API using FastAPI or Flask

UI for user to input custom text and get classification
